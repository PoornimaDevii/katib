{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID-19 FORECAST USING KUBEFLOW FAIRING ON UCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone Cisco Kubeflow starter pack repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRANCH_NAME=\"dev\" #Provide git branch \"master\" or \"dev\"\n",
    "! git clone -b $BRANCH_NAME https://github.com/CiscoAI/cisco-kubeflow-starter-pack.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Docker Credentials\n",
    "\n",
    "Get your docker registry user name and password encoded in base64 \n",
    "\n",
    "echo -n USERNAME:PASSWORD | base64 \n",
    "\n",
    "Create a config.json file with your Docker registry url and the previous generated base64 string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo -n USERNAME:PASSWORD | base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.json\n",
    "{\n",
    "    \"auths\": {\n",
    "        \"https://index.docker.io/v1/\": {\n",
    "            \"auth\": \"<<Provide previous generated base64 string>>\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "pandas\n",
    "keras\n",
    "seldon-core\n",
    "tornado>=6.0.3\n",
    "kubeflow-fairing\n",
    "tensorflow==1.13.1\n",
    "cloudpickle==1.1.1\n",
    "kubernetes==10.0.1\n",
    "matplotlib\n",
    "plotly_express"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/jovyan/.local/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.0.3)\n",
      "Requirement already satisfied: keras in /home/jovyan/.local/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (2.3.1)\n",
      "Requirement already satisfied: seldon-core in /home/jovyan/.local/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /home/jovyan/.local/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (6.0.4)\n",
      "Requirement already satisfied: kubeflow-fairing in /home/jovyan/.local/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (0.7.2)\n",
      "Requirement already satisfied: tensorflow==1.13.1 in /home/jovyan/.local/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (1.13.1)\n",
      "Requirement already satisfied: cloudpickle==1.1.1 in /home/jovyan/.local/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (1.1.1)\n",
      "Requirement already satisfied: kubernetes==10.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (10.0.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (3.1.2)\n",
      "Requirement already satisfied: plotly_express in /home/jovyan/.local/lib/python3.6/site-packages (from -r requirements.txt (line 10)) (0.4.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 1)) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->-r requirements.txt (line 2)) (1.0.8)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->-r requirements.txt (line 2)) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->-r requirements.txt (line 2)) (5.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->-r requirements.txt (line 2)) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->-r requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from keras->-r requirements.txt (line 2)) (1.11.0)\n",
      "Requirement already satisfied: Flask<2.0.0 in /home/jovyan/.local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 3)) (1.1.2)\n",
      "Requirement already satisfied: opentracing<2.3.0,>=2.2.0 in /home/jovyan/.local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 3)) (2.2.0)\n",
      "Requirement already satisfied: pyaml<20.0.0 in /home/jovyan/.local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 3)) (19.12.0)\n",
      "Requirement already satisfied: azure-storage-blob<3.0.0,>=2.0.1 in /home/jovyan/.local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: prometheus-client<0.8.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from seldon-core->-r requirements.txt (line 3)) (0.7.1)\n",
      "Requirement already satisfied: grpcio<2.0.0 in /usr/local/lib/python3.6/dist-packages (from seldon-core->-r requirements.txt (line 3)) (1.26.0)\n",
      "Requirement already satisfied: redis<4.0.0 in /home/jovyan/.local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 3)) (3.5.0)\n",
      "Requirement already satisfied: gunicorn<20.1.0,>=19.9.0 in /home/jovyan/.local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 3)) (20.0.4)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.6/dist-packages (from seldon-core->-r requirements.txt (line 3)) (3.11.2)\n",
      "Requirement already satisfied: flatbuffers<2.0.0 in /home/jovyan/.local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: Flask-OpenTracing<1.2.0,>=1.1.0 in /home/jovyan/.local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: jaeger-client<4.2.0,>=4.1.0 in /home/jovyan/.local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 3)) (4.1.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from seldon-core->-r requirements.txt (line 3)) (45.1.0)\n",
      "Requirement already satisfied: minio<6.0.0,>=4.0.9 in /home/jovyan/.local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 3)) (5.0.10)\n",
      "Requirement already satisfied: Flask-cors<4.0.0 in /home/jovyan/.local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 3)) (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.6/dist-packages (from seldon-core->-r requirements.txt (line 3)) (2.22.0)\n",
      "Requirement already satisfied: grpcio-opentracing<1.2.0,>=1.1.4 in /home/jovyan/.local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 3)) (1.1.4)\n",
      "Requirement already satisfied: urllib3==1.24.2 in /home/jovyan/.local/lib/python3.6/site-packages (from kubeflow-fairing->-r requirements.txt (line 5)) (1.24.2)\n",
      "Requirement already satisfied: google-cloud-logging>=1.13.0 in /home/jovyan/.local/lib/python3.6/site-packages (from kubeflow-fairing->-r requirements.txt (line 5)) (1.15.0)\n",
      "Requirement already satisfied: azure-mgmt-storage>=9.0.0 in /home/jovyan/.local/lib/python3.6/site-packages (from kubeflow-fairing->-r requirements.txt (line 5)) (9.0.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /home/jovyan/.local/lib/python3.6/site-packages (from kubeflow-fairing->-r requirements.txt (line 5)) (1.3.3)\n",
      "Requirement already satisfied: kubeflow-pytorchjob>=0.1.1 in /home/jovyan/.local/lib/python3.6/site-packages (from kubeflow-fairing->-r requirements.txt (line 5)) (0.1.3)\n",
      "Requirement already satisfied: kfserving>=0.2.1.1 in /home/jovyan/.local/lib/python3.6/site-packages (from kubeflow-fairing->-r requirements.txt (line 5)) (0.3.0.1)\n",
      "Requirement already satisfied: azure-storage-file>=2.1.0 in /home/jovyan/.local/lib/python3.6/site-packages (from kubeflow-fairing->-r requirements.txt (line 5)) (2.1.0)\n",
      "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from kubeflow-fairing->-r requirements.txt (line 5)) (0.18.2)\n",
      "Requirement already satisfied: httplib2>=0.12.0 in /usr/local/lib/python3.6/dist-packages (from kubeflow-fairing->-r requirements.txt (line 5)) (0.17.0)\n",
      "Requirement already satisfied: boto3>=1.9.0 in /home/jovyan/.local/lib/python3.6/site-packages (from kubeflow-fairing->-r requirements.txt (line 5)) (1.13.3)\n",
      "Requirement already satisfied: google-api-python-client>=1.7.8 in /usr/local/lib/python3.6/dist-packages (from kubeflow-fairing->-r requirements.txt (line 5)) (1.7.11)\n",
      "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from kubeflow-fairing->-r requirements.txt (line 5)) (4.1.3)\n",
      "Requirement already satisfied: notebook>=5.6.0 in /usr/local/lib/python3.6/dist-packages (from kubeflow-fairing->-r requirements.txt (line 5)) (6.0.3)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.2 in /usr/local/lib/python3.6/dist-packages (from kubeflow-fairing->-r requirements.txt (line 5)) (1.25.0)\n",
      "Requirement already satisfied: ibm-cos-sdk>=2.6.0 in /home/jovyan/.local/lib/python3.6/site-packages (from kubeflow-fairing->-r requirements.txt (line 5)) (2.6.2)\n",
      "Requirement already satisfied: docker>=3.4.1 in /usr/local/lib/python3.6/dist-packages (from kubeflow-fairing->-r requirements.txt (line 5)) (4.1.0)\n",
      "Requirement already satisfied: kubeflow-tfjob>=0.1.1 in /home/jovyan/.local/lib/python3.6/site-packages (from kubeflow-fairing->-r requirements.txt (line 5)) (0.1.3)\n",
      "Requirement already satisfied: google-auth>=1.6.2 in /usr/local/lib/python3.6/dist-packages (from kubeflow-fairing->-r requirements.txt (line 5)) (1.11.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 6)) (0.9.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 6)) (0.2.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 6)) (0.8.1)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow==1.13.1->-r requirements.txt (line 6)) (1.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 6)) (1.1.0)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow==1.13.1->-r requirements.txt (line 6)) (1.13.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow==1.13.1->-r requirements.txt (line 6)) (0.30.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.6/dist-packages (from kubernetes==10.0.1->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.6/dist-packages (from kubernetes==10.0.1->-r requirements.txt (line 8)) (2019.11.28)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes==10.0.1->-r requirements.txt (line 8)) (0.57.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 9)) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 9)) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 9)) (2.4.6)\n",
      "Requirement already satisfied: patsy>=0.5 in /home/jovyan/.local/lib/python3.6/site-packages (from plotly_express->-r requirements.txt (line 10)) (0.5.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /home/jovyan/.local/lib/python3.6/site-packages (from plotly_express->-r requirements.txt (line 10)) (0.11.1)\n",
      "Requirement already satisfied: plotly>=4.1.0 in /home/jovyan/.local/lib/python3.6/site-packages (from plotly_express->-r requirements.txt (line 10)) (4.6.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask<2.0.0->seldon-core->-r requirements.txt (line 3)) (0.16.1)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /home/jovyan/.local/lib/python3.6/site-packages (from Flask<2.0.0->seldon-core->-r requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask<2.0.0->seldon-core->-r requirements.txt (line 3)) (2.11.0)\n",
      "Requirement already satisfied: click>=5.1 in /home/jovyan/.local/lib/python3.6/site-packages (from Flask<2.0.0->seldon-core->-r requirements.txt (line 3)) (7.1.2)\n",
      "Requirement already satisfied: azure-storage-common~=2.1 in /home/jovyan/.local/lib/python3.6/site-packages (from azure-storage-blob<3.0.0,>=2.0.1->seldon-core->-r requirements.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: azure-common>=1.1.5 in /home/jovyan/.local/lib/python3.6/site-packages (from azure-storage-blob<3.0.0,>=2.0.1->seldon-core->-r requirements.txt (line 3)) (1.1.25)\n",
      "Requirement already satisfied: threadloop<2,>=1 in /home/jovyan/.local/lib/python3.6/site-packages (from jaeger-client<4.2.0,>=4.1.0->seldon-core->-r requirements.txt (line 3)) (1.0.2)\n",
      "Requirement already satisfied: thrift in /home/jovyan/.local/lib/python3.6/site-packages (from jaeger-client<4.2.0,>=4.1.0->seldon-core->-r requirements.txt (line 3)) (0.13.0)\n",
      "Requirement already satisfied: configparser in /home/jovyan/.local/lib/python3.6/site-packages (from minio<6.0.0,>=4.0.9->seldon-core->-r requirements.txt (line 3)) (5.0.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0->seldon-core->-r requirements.txt (line 3)) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->seldon-core->-r requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-logging>=1.13.0->kubeflow-fairing->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-logging>=1.13.0->kubeflow-fairing->-r requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: msrestazure<2.0.0,>=0.4.32 in /home/jovyan/.local/lib/python3.6/site-packages (from azure-mgmt-storage>=9.0.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.6.3)\n",
      "Requirement already satisfied: msrest>=0.5.0 in /home/jovyan/.local/lib/python3.6/site-packages (from azure-mgmt-storage>=9.0.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.6.13)\n",
      "Requirement already satisfied: table-logger>=0.3.5 in /home/jovyan/.local/lib/python3.6/site-packages (from kubeflow-pytorchjob>=0.1.1->kubeflow-fairing->-r requirements.txt (line 5)) (0.3.6)\n",
      "Requirement already satisfied: argparse>=1.4.0 in /home/jovyan/.local/lib/python3.6/site-packages (from kfserving>=0.2.1.1->kubeflow-fairing->-r requirements.txt (line 5)) (1.4.0)\n",
      "Requirement already satisfied: adal>=1.2.2 in /home/jovyan/.local/lib/python3.6/site-packages (from kfserving>=0.2.1.1->kubeflow-fairing->-r requirements.txt (line 5)) (1.2.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/jovyan/.local/lib/python3.6/site-packages (from boto3>=1.9.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.9.5)\n",
      "Requirement already satisfied: botocore<1.17.0,>=1.16.3 in /home/jovyan/.local/lib/python3.6/site-packages (from boto3>=1.9.0->kubeflow-fairing->-r requirements.txt (line 5)) (1.16.3)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/jovyan/.local/lib/python3.6/site-packages (from boto3>=1.9.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.3.3)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.7.8->kubeflow-fairing->-r requirements.txt (line 5)) (0.0.3)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.7.8->kubeflow-fairing->-r requirements.txt (line 5)) (3.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.2.8)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.4.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->kubeflow-fairing->-r requirements.txt (line 5)) (4.0)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (5.1.4)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (4.3.3)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.8.3)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (1.5.0)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (5.0.4)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (5.6.1)\n",
      "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (4.6.1)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.6/dist-packages (from notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (18.1.1)\n",
      "Requirement already satisfied: jupyter-client>=5.3.4 in /usr/local/lib/python3.6/dist-packages (from notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (5.3.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.2->kubeflow-fairing->-r requirements.txt (line 5)) (0.5.0)\n",
      "Requirement already satisfied: ibm-cos-sdk-core==2.6.2 in /home/jovyan/.local/lib/python3.6/site-packages (from ibm-cos-sdk>=2.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (2.6.2)\n",
      "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.6.2 in /home/jovyan/.local/lib/python3.6/site-packages (from ibm-cos-sdk>=2.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (2.6.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.2->kubeflow-fairing->-r requirements.txt (line 5)) (4.0.0)\n",
      "Requirement already satisfied: mock>=2.0.0 in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1->-r requirements.txt (line 6)) (4.0.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1->-r requirements.txt (line 6)) (3.1.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->kubernetes==10.0.1->-r requirements.txt (line 8)) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask<2.0.0->seldon-core->-r requirements.txt (line 3)) (1.1.1)\n",
      "Requirement already satisfied: cryptography in /usr/lib/python3/dist-packages (from azure-storage-common~=2.1->azure-storage-blob<3.0.0,>=2.0.1->seldon-core->-r requirements.txt (line 3)) (2.1.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-logging>=1.13.0->kubeflow-fairing->-r requirements.txt (line 5)) (1.51.0)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /home/jovyan/.local/lib/python3.6/site-packages (from msrest>=0.5.0->azure-mgmt-storage>=9.0.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.6.0)\n",
      "Requirement already satisfied: PyJWT>=1.0.0 in /home/jovyan/.local/lib/python3.6/site-packages (from adal>=1.2.2->kfserving>=0.2.1.1->kubeflow-fairing->-r requirements.txt (line 5)) (1.7.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/jovyan/.local/lib/python3.6/site-packages (from botocore<1.17.0,>=1.16.3->boto3>=1.9.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.15.2)\n",
      "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (7.11.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (4.4.1)\n",
      "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.6.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (3.2.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.3)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.6.0)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (3.1.0)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.4.4)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (1.4.2)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (2.5.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (2.0.10)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.16.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (4.8.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.6/dist-packages (from ipython>=5.0.0->ipykernel->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.1.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (19.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (1.4.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.15.7)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.5.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.1.8)\n",
      "Requirement already satisfied: parso>=0.5.2 in /usr/local/lib/python3.6/dist-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (0.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->kubeflow-fairing->-r requirements.txt (line 5)) (2.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart Notebook Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_html\n",
    "display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jovyan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jovyan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jovyan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jovyan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jovyan/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re,os\n",
    "import logging\n",
    "import sys\n",
    "import importlib\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras import optimizers\n",
    "\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Kubeflow Fairing for training and predictions on On-premise\n",
    "Import the fairing library and configure the onprem environment that your training or prediction job will run in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace : anonymous\n"
     ]
    }
   ],
   "source": [
    "from kubernetes import client as k8s_client\n",
    "from kubernetes.client import rest as k8s_rest\n",
    "from kubernetes import config as k8s_config\n",
    "from kubernetes.client.rest import ApiException\n",
    "\n",
    "from kubeflow import fairing   \n",
    "from kubeflow.fairing import utils as fairing_utils\n",
    "from kubeflow.fairing import TrainJob\n",
    "from kubeflow.fairing.preprocessors.function import FunctionPreProcessor\n",
    "from kubeflow.fairing.preprocessors import base as base_preprocessor\n",
    "from kubeflow.fairing.builders.cluster.cluster import ClusterBuilder\n",
    "\n",
    "from kubeflow.fairing.cloud.k8s import MinioUploader\n",
    "from kubeflow.fairing.builders.cluster.minio_context import MinioContextSource\n",
    "from kubeflow.fairing import PredictionEndpoint\n",
    "from kubeflow.fairing.kubernetes.utils import mounting_pvc\n",
    "from kubeflow.fairing.kubernetes.utils import mounting_pvc\n",
    "\n",
    "BackendClass = getattr(importlib.import_module('kubeflow.fairing.backends'), \"KubernetesBackend\")\n",
    "namespace = fairing_utils.get_current_k8s_namespace()\n",
    "print(\"Namespace : %s\"%namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get minio-service cluster IP to upload docker build context\n",
    "#### Set DOCKER_REGISTRY\n",
    "The DOCKER_REGISTRY variable is used to push the newly built image. \n",
    "Please change the variable to the registry for which you've configured credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://10.98.188.38:9000\n"
     ]
    }
   ],
   "source": [
    "DOCKER_REGISTRY = \"poornimadevii\"\n",
    "\n",
    "k8s_config.load_incluster_config()\n",
    "api_client = k8s_client.CoreV1Api()\n",
    "custom_api=k8s_client.CustomObjectsApi()\n",
    "minio_service_endpoint = None\n",
    "\n",
    "try:\n",
    "    minio_service_endpoint = api_client.read_namespaced_service(name='minio-service', namespace='kubeflow').spec.cluster_ip\n",
    "except ApiException as e:\n",
    "    if e.status == 403:\n",
    "        logging.warning(f\"The service account doesn't have sufficient privileges \"\n",
    "                      f\"to get the kubeflow minio-service. \"\n",
    "                      f\"You will have to manually enter the minio cluster-ip. \"\n",
    "                      f\"To make this function work ask someone with cluster \"\n",
    "                      f\"priveleges to create an appropriate \"\n",
    "                      f\"clusterrolebinding by running a command.\\n\"\n",
    "                      f\"kubectl create --namespace=kubeflow rolebinding \"\n",
    "                       \"--clusterrole=kubeflow-view \"\n",
    "                       \"--serviceaccount=${NAMESPACE}:default-editor \"\n",
    "                       \"${NAMESPACE}-minio-view\")\n",
    "        logging.error(\"API access denied with reason: {e.reason}\")\n",
    "\n",
    "s3_endpoint = minio_service_endpoint\n",
    "minio_endpoint = \"http://\"+s3_endpoint+\":9000\"\n",
    "minio_username = \"minio\"\n",
    "minio_key = \"minio123\"\n",
    "minio_region = \"us-east-1\"\n",
    "print(minio_endpoint)\n",
    "\n",
    "\n",
    "minio_uploader = MinioUploader(endpoint_url=minio_endpoint, minio_secret=minio_username, minio_secret_key=minio_key, region_name=minio_region)\n",
    "minio_context_source = MinioContextSource(endpoint_url=minio_endpoint, minio_secret=minio_username, minio_secret_key=minio_key, region_name=minio_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a config-map in the namespace you're using with the docker config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create --namespace $namespace configmap docker-config --from-file=./config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define required paths to train & test data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = 'cisco-kubeflow-starter-pack/apps/healthcare/covid-forecasting/onprem/'\n",
    "train_data_path = main_path + 'data/train.csv'\n",
    "test_data_path = main_path + 'data/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200513 09:29:42 cluster:46] Building image using cluster builder.\n",
      "[I 200513 09:29:42 base:107] Creating docker context: /tmp/fairing_context_uukzpy8g\n",
      "[W 200513 09:29:42 base:94] /tmp/fairing_dockerfile_w09jkknb already exists in Fairing context, skipping...\n",
      "[W 200513 09:29:43 manager:296] Waiting for fairing-builder-pfgt5-wgrkg to start...\n",
      "[W 200513 09:29:43 manager:296] Waiting for fairing-builder-pfgt5-wgrkg to start...\n",
      "[W 200513 09:29:43 manager:296] Waiting for fairing-builder-pfgt5-wgrkg to start...\n",
      "[W 200513 09:29:45 manager:296] Waiting for fairing-builder-pfgt5-wgrkg to start...\n",
      "[I 200513 09:29:52 manager:302] Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mINFO\u001b[0m[0001] Resolved base name python:3.7-slim-buster to python:3.7-slim-buster\n",
      "\u001b[36mINFO\u001b[0m[0001] Resolved base name python:3.7-slim-buster to python:3.7-slim-buster\n",
      "\u001b[36mINFO\u001b[0m[0001] Downloading base image python:3.7-slim-buster\n",
      "\u001b[36mINFO\u001b[0m[0002] Error while retrieving image from cache: getting file info: stat /cache/sha256:0f322e5066a6c5c643829739dc93ea8ab73204abdea63b15af700fe6efd2ce4f: no such file or directory\n",
      "\u001b[36mINFO\u001b[0m[0002] Downloading base image python:3.7-slim-buster\n",
      "\u001b[36mINFO\u001b[0m[0003] Built cross stage deps: map[]\n",
      "\u001b[36mINFO\u001b[0m[0003] Downloading base image python:3.7-slim-buster\n",
      "\u001b[36mINFO\u001b[0m[0004] Error while retrieving image from cache: getting file info: stat /cache/sha256:0f322e5066a6c5c643829739dc93ea8ab73204abdea63b15af700fe6efd2ce4f: no such file or directory\n",
      "\u001b[36mINFO\u001b[0m[0004] Downloading base image python:3.7-slim-buster\n",
      "\u001b[36mINFO\u001b[0m[0004] Unpacking rootfs as cmd RUN pip install tensorflow==1.13.1 --no-cache-dir requires it.\n",
      "\u001b[36mINFO\u001b[0m[0009] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0010] RUN pip install tensorflow==1.13.1 --no-cache-dir\n",
      "\u001b[36mINFO\u001b[0m[0010] cmd: /bin/sh\n",
      "\u001b[36mINFO\u001b[0m[0010] args: [-c pip install tensorflow==1.13.1 --no-cache-dir]\n",
      "Collecting tensorflow==1.13.1\n",
      "  Downloading tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6 MB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.28.1-cp37-cp37m-manylinux2010_x86_64.whl (2.8 MB)\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
      "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
      "Collecting numpy>=1.13.3\n",
      "  Downloading numpy-1.18.4-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n",
      "Collecting keras-applications>=1.0.6\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Collecting six>=1.10.0\n",
      "  Downloading six-1.14.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.13.1) (0.34.2)\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting keras-preprocessing>=1.0.5\n",
      "  Downloading Keras_Preprocessing-1.1.1-py2.py3-none-any.whl (42 kB)\n",
      "Collecting protobuf>=3.6.1\n",
      "  Downloading protobuf-3.11.3-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tensorboard<1.14.0,>=1.13.0\n",
      "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
      "Collecting absl-py>=0.1.6\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "Collecting gast>=0.2.0\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting mock>=2.0.0\n",
      "  Downloading mock-4.0.2-py3-none-any.whl (28 kB)\n",
      "Collecting h5py\n",
      "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (46.1.3)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting importlib-metadata; python_version < \"3.8\"\n",
      "  Downloading importlib_metadata-1.6.0-py2.py3-none-any.whl (30 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.1.0-py3-none-any.whl (4.9 kB)\n",
      "Building wheels for collected packages: termcolor, absl-py\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=04fe2423800cb7cc4fda9bf3fc037323774965f86798a0b5677a798bfa96f9eb\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1saxicc6/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=8dc84cc1fd7897e479c11978b85c28c1a289b03c2e66ca410ad9c4a8e8b2673b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1saxicc6/wheels/cc/af/1a/498a24d0730ef484019e007bb9e8cef3ac00311a672c049a3e\n",
      "Successfully built termcolor absl-py\n",
      "Installing collected packages: six, grpcio, mock, numpy, absl-py, tensorflow-estimator, h5py, keras-applications, astor, keras-preprocessing, protobuf, termcolor, zipp, importlib-metadata, markdown, werkzeug, tensorboard, gast, tensorflow\n",
      "Successfully installed absl-py-0.9.0 astor-0.8.1 gast-0.3.3 grpcio-1.28.1 h5py-2.10.0 importlib-metadata-1.6.0 keras-applications-1.0.8 keras-preprocessing-1.1.1 markdown-3.2.2 mock-4.0.2 numpy-1.18.4 protobuf-3.11.3 six-1.14.0 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0 werkzeug-1.0.1 zipp-3.1.0\n",
      "\u001b[36mINFO\u001b[0m[0030] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0058] RUN pip install keras --no-cache-dir\n",
      "\u001b[36mINFO\u001b[0m[0058] cmd: /bin/sh\n",
      "\u001b[36mINFO\u001b[0m[0058] args: [-c pip install keras --no-cache-dir]\n",
      "Collecting keras\n",
      "  Downloading Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/site-packages (from keras) (1.14.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/site-packages (from keras) (1.1.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/site-packages (from keras) (1.0.8)\n",
      "Collecting scipy>=0.14\n",
      "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/site-packages (from keras) (1.18.4)\n",
      "Building wheels for collected packages: pyyaml\n",
      "  Building wheel for pyyaml (setup.py): started\n",
      "  Building wheel for pyyaml (setup.py): finished with status 'done'\n",
      "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44619 sha256=0c45aaed46b64c1085fb0f6634a672ca021df3329a5729abd219ee0d71250cdf\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-y2i__uqj/wheels/5e/03/1e/e1e954795d6f35dfc7b637fe2277bff021303bd9570ecea653\n",
      "Successfully built pyyaml\n",
      "Installing collected packages: pyyaml, scipy, keras\n",
      "Successfully installed keras-2.3.1 pyyaml-5.3.1 scipy-1.4.1\n",
      "\u001b[36mINFO\u001b[0m[0063] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0072] RUN pip install pandas\n",
      "\u001b[36mINFO\u001b[0m[0072] cmd: /bin/sh\n",
      "\u001b[36mINFO\u001b[0m[0072] args: [-c pip install pandas]\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.0.3-cp37-cp37m-manylinux1_x86_64.whl (10.0 MB)\n",
      "Collecting python-dateutil>=2.6.1\n",
      "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/site-packages (from pandas) (1.18.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "Installing collected packages: python-dateutil, pytz, pandas\n",
      "Successfully installed pandas-1.0.3 python-dateutil-2.8.1 pytz-2020.1\n",
      "\u001b[36mINFO\u001b[0m[0076] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0082] Using files from context: [/kaniko/buildcontext/covid-model.py]\n",
      "\u001b[36mINFO\u001b[0m[0082] ADD covid-model.py  /opt/covid-model.py\n",
      "\u001b[36mINFO\u001b[0m[0082] Taking snapshot of files...\n",
      "\u001b[36mINFO\u001b[0m[0082] Using files from context: [/kaniko/buildcontext/train.csv]\n",
      "\u001b[36mINFO\u001b[0m[0082] ADD train.csv /opt/train.csv\n",
      "\u001b[36mINFO\u001b[0m[0082] Taking snapshot of files...\n",
      "\u001b[36mINFO\u001b[0m[0082] Using files from context: [/kaniko/buildcontext/test.csv]\n",
      "\u001b[36mINFO\u001b[0m[0082] ADD test.csv /opt/test.csv\n",
      "\u001b[36mINFO\u001b[0m[0082] Taking snapshot of files...\n",
      "\u001b[36mINFO\u001b[0m[0082] RUN chmod +x /opt/*\n",
      "\u001b[36mINFO\u001b[0m[0082] cmd: /bin/sh\n",
      "\u001b[36mINFO\u001b[0m[0082] args: [-c chmod +x /opt/*]\n",
      "\u001b[36mINFO\u001b[0m[0082] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0084] ENTRYPOINT [\"/usr/local/bin/python3\"]\n",
      "\u001b[36mINFO\u001b[0m[0084] CMD [\"/opt/covid-model.py\"]\n"
     ]
    }
   ],
   "source": [
    "output_map= {\n",
    "    os.path.join(main_path, \"fairing/Dockerfile\"): \"Dockerfile\",\n",
    "    os.path.join(main_path, \"fairing/covid-model.py\"):\"covid-model.py\",\n",
    "    os.path.join(train_data_path): \"train.csv\",\n",
    "    os.path.join(test_data_path): \"test.csv\"\n",
    "}\n",
    "\n",
    "# output_map= {\n",
    "#       \"Dockerfile\": \"Dockerfile\",\n",
    "#       \"covid-model.py\" : \"covid-model.py\",\n",
    "#       \"train.csv\" : \"train.csv\",\n",
    "#       \"test.csv\" : \"test.csv\"\n",
    "# }\n",
    "\n",
    "preprocessor = base_preprocessor.BasePreProcessor(output_map=output_map, input_files=['requirements.txt'])\n",
    "\n",
    "preprocessor.preprocess()\n",
    "builder = ClusterBuilder(registry=DOCKER_REGISTRY, preprocessor=preprocessor, context_source=minio_context_source)\n",
    "\n",
    "builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'poornimadevii/fairing-job:EF894B62'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.image_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Katib Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apiVersion': 'kubeflow.org/v1alpha3',\n",
       " 'kind': 'Experiment',\n",
       " 'metadata': {'creationTimestamp': '2020-05-13T09:47:27Z',\n",
       "  'generation': 1,\n",
       "  'labels': {'controller-tools.k8s.io': '1.0'},\n",
       "  'name': 'covid4',\n",
       "  'namespace': 'kubeflow',\n",
       "  'resourceVersion': '9038772',\n",
       "  'selfLink': '/apis/kubeflow.org/v1alpha3/namespaces/kubeflow/experiments/covid4',\n",
       "  'uid': '56046c00-0ed7-4cf6-ab68-957360cb8e9b'},\n",
       " 'spec': {'algorithm': {'algorithmName': 'bayesianoptimization'},\n",
       "  'maxFailedTrialCount': 3,\n",
       "  'maxTrialCount': 5,\n",
       "  'objective': {'goal': 0.99,\n",
       "   'objectiveMetricName': 'Validation-accuracy',\n",
       "   'type': 'maximize'},\n",
       "  'parallelTrialCount': 5,\n",
       "  'parameters': [{'feasibleSpace': {'list': ['16', '32', '48', '64']},\n",
       "    'name': '--batch-size',\n",
       "    'parameterType': 'categorical'},\n",
       "   {'feasibleSpace': {'list': ['adam', 'sgd']},\n",
       "    'name': '--optimizer',\n",
       "    'parameterType': 'categorical'}],\n",
       "  'trialTemplate': {'goTemplate': {'rawTemplate': 'apiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: {{.Trial}}\\n  namespace: {{.NameSpace}}\\nspec:\\n  template:\\n    spec:\\n      containers:\\n      - name: {{.Trial}}\\n        image: poornimadevii/fairing-job:EF894B62\\n        command:\\n        - \"python3\"\\n        - \"/opt/covid-model.py\"\\n        {{- with .HyperParameters}}\\n        {{- range .}}\\n        - \"{{.Name}}={{.Value}}\"\\n        {{- end}}\\n        {{- end}}\\n      restartPolicy: Never'}}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Covid example\n",
    "experiment={\n",
    "  \"apiVersion\": \"kubeflow.org/v1alpha3\",\n",
    "  \"kind\": \"Experiment\",\n",
    "  \"metadata\": {\n",
    "    \"namespace\": \"kubeflow\",\n",
    "    \"labels\": {\n",
    "      \"controller-tools.k8s.io\": \"1.0\"\n",
    "     \n",
    "    },\n",
    "    \"name\": \"covid4\"\n",
    "  },\n",
    "  \"spec\": {\n",
    "    \"objective\": {\n",
    "      \"type\": \"maximize\",\n",
    "      \"goal\": 0.99,\n",
    "      \"objectiveMetricName\": \"Validation-accuracy\",\n",
    "#       \"additionalMetricNames\": [\n",
    "#         \"Train-accuracy\"\n",
    "#       ]\n",
    "  },\n",
    "    \"algorithm\": {\n",
    "      \"algorithmName\": \"bayesianoptimization\"\n",
    "    },\n",
    "    \"parallelTrialCount\": 5,\n",
    "    \"maxTrialCount\": 5,\n",
    "    \"maxFailedTrialCount\": 3,\n",
    "    \"parameters\": [\n",
    "      {\n",
    "        \"name\": \"--batch-size\",\n",
    "        \"parameterType\": \"categorical\",\n",
    "        \"feasibleSpace\": {\n",
    "          \"list\": [\n",
    "            \"16\",\n",
    "            \"32\",\n",
    "            \"48\",\n",
    "            \"64\"\n",
    "          ]\n",
    "        }\n",
    "      },\n",
    "#         {\n",
    "#         \"name\": \"--epochs\",\n",
    "#         \"parameterType\": \"categorical\",\n",
    "#         \"feasibleSpace\": {\n",
    "#           \"list\": [\n",
    "#             \"5\",\n",
    "#             \"10\",\n",
    "#             \"15\",\n",
    "#             \"20\"\n",
    "#           ]\n",
    "#         }\n",
    "#       }\n",
    "     \n",
    "     {\n",
    "        \"name\": \"--optimizer\",\n",
    "        \"parameterType\": \"categorical\",\n",
    "        \"feasibleSpace\": {\n",
    "          \"list\": [\n",
    "            \"adam\",\n",
    "            \"sgd\"\n",
    "          ]\n",
    "        }\n",
    "      }    \n",
    "    ],\n",
    "    \"trialTemplate\": {\n",
    "      \"goTemplate\": {\n",
    "        \"rawTemplate\": \"apiVersion: batch/v1\\nkind: Job\\nmetadata:\\n  name: {{.Trial}}\\n  namespace: {{.NameSpace}}\\nspec:\\n  template:\\n    spec:\\n      containers:\\n      - name: {{.Trial}}\\n        image: %s\\n        command:\\n        - \\\"python3\\\"\\n        - \\\"/opt/covid-model.py\\\"\\n        {{- with .HyperParameters}}\\n        {{- range .}}\\n        - \\\"{{.Name}}={{.Value}}\\\"\\n        {{- end}}\\n        {{- end}}\\n      restartPolicy: Never\"%builder.image_tag\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "experiment_name=experiment[\"metadata\"][\"name\"]\n",
    "custom_api.create_namespaced_custom_object(group=\"kubeflow.org\", version=\"v1alpha3\", namespace=\"kubeflow\", plural=\"experiments\", body=experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mnist example\n",
    "\n",
    "experiment={\n",
    "  \"apiVersion\": \"kubeflow.org/v1alpha3\",\n",
    "  \"kind\": \"Experiment\",\n",
    "  \"metadata\": {\n",
    "    \"namespace\": \"anonymous\",\n",
    "    \"labels\": {\n",
    "      \"controller-tools.k8s.io\": \"1.0\"\n",
    "    },\n",
    "    \"name\": \"mnist\"\n",
    "  },\n",
    "  \"spec\": {\n",
    "    \"objective\": {\n",
    "      \"type\": \"maximize\",\n",
    "      \"goal\": 0.99,\n",
    "      \"objectiveMetricName\": \"Validation-accuracy\",\n",
    "  },\n",
    "    \"algorithm\": {\n",
    "      \"algorithmName\": \"bayesianoptimization\"\n",
    "    },\n",
    "    \"parallelTrialCount\": 5,\n",
    "    \"maxTrialCount\": 5,\n",
    "    \"maxFailedTrialCount\": 3,\n",
    "    \"parameters\": [\n",
    "      {\n",
    "        \"name\": \"--lr\",\n",
    "        \"parameterType\": \"double\",\n",
    "        \"feasibleSpace\": {\n",
    "          \"min\": \"0.01\",\n",
    "          \"max\": \"0.03\"\n",
    "        }\n",
    "      },\n",
    "        {\n",
    "        \"name\": \"--num-layers\",\n",
    "        \"parameterType\": \"int\",\n",
    "        \"feasibleSpace\": {\n",
    "          \"min\": \"2\",\n",
    "          \"max\": \"5\"\n",
    "        }\n",
    "      },\n",
    "     {\n",
    "        \"name\": \"--optimizer\",\n",
    "        \"parameterType\": \"categorical\",\n",
    "        \"feasibleSpace\": {\n",
    "          \"list\": [\n",
    "            \"adam\",\n",
    "            \"sgd\",\n",
    "            \"ftrl\"\n",
    "          ]\n",
    "        }\n",
    "      }    ],\n",
    "    \"trialTemplate\": {\n",
    "      \"goTemplate\": {\n",
    "        \"rawTemplate\": \"apiVersion: \\\"batch/v1\\\"\\nkind: Job\\nmetadata:\\n  name: {{.Trial}}\\n  namespace: {{.NameSpace}}\\nspec:\\n  template:\\n    spec:\\n      containers:\\n      - name: {{.Trial}}\\n        image: docker.io/kubeflowkatib/mxnet-mnist\\n        command:\\n        - \\\"python3\\\"\\n        - \\\"/opt/mxnet-mnist/mnist.py\\\"\\n        - \\\"--batch-size=64\\\"\\n        {{- with .HyperParameters}}\\n        {{- range .}}\\n        - \\\"{{.Name}}={{.Value}}\\\"\\n        {{- end}}\\n        {{- end}}\\n      restartPolicy: Never\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "experiment_name=experiment[\"metadata\"][\"name\"]\n",
    "custom_api.create_namespaced_custom_object(group=\"kubeflow.org\", version=\"v1alpha3\", namespace=namespace, plural=\"experiments\", body=experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for experiment succeeded status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status=False\n",
    "while True:\n",
    "    conditions=conditions=custom_api.get_namespaced_custom_object_status(group=\"kubeflow.org\", version=\"v1alpha3\", namespace=namespace, plural=\"experiments\", name=experiment[\"metadata\"][\"name\"])[\"status\"][\"conditions\"]\n",
    "    for i in range(len(conditions)):\n",
    "        if (conditions[i]['type'])=='Succeeded':\n",
    "            status=True\n",
    "            print(\"Experiment Status: %s\"%conditions[i]['type'])\n",
    "            break\n",
    "        \n",
    "    if status:\n",
    "        break\n",
    "    print(\"Experiment Status: %s\"%conditions[i]['type'])\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function for preprocessing train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train(train_df, n_prev, n_next):\n",
    "    df = train_df.copy()\n",
    "    input_feats, output_feats = [], []\n",
    "    \n",
    "    # Performing Shifting of Previous cases in the positive direction (downwards) for New cases & New Fatalities\n",
    "    for i in range(1, n_prev+1):\n",
    "        for feat in [\"NewCases\", \"NewFatalities\"]:\n",
    "            df[\"{}_prev_{}\".format(feat, i)] = df.groupby([\"Country_Region\", \"Province_State\"])[feat].shift(i)\n",
    "            input_feats.append(\"{}_prev_{}\".format(feat, i))\n",
    "    \n",
    "    # Performing Shifting of Next Cases in the negative direction (upwards) for New cases & New Fatalities\n",
    "    output_feats.extend([\"NewCases\", \"NewFatalities\"])\n",
    "    for i in range(1, n_next):\n",
    "        for feat in [\"NewCases\", \"NewFatalities\"]:\n",
    "            df[\"{}_next_{}\".format(feat, i)] = df.groupby([\"Country_Region\", \"Province_State\"])[feat].shift(-i)\n",
    "            output_feats.append(\"{}_next_{}\".format(feat, i))\n",
    "    df.dropna(inplace=True)     \n",
    "    \n",
    "    #Converting the Province state & Country Region to Dummy/Indicator Variables ( which is a constant)\n",
    "    const_df = pd.get_dummies(df[[\"Province_State\", \"Country_Region\"]], drop_first=True)\n",
    "    \n",
    "    # Assigning already available data for previous no of days counting back from starting date of forecasting dates\n",
    "    time_df = df[input_feats]\n",
    "    time_df = time_df.values.reshape((df.shape[0],-1,2))\n",
    "    \n",
    "    #Assigning values to the future no of days counting forth from the starting date of forecasting dates\n",
    "    output_df = df[output_feats]\n",
    "    return const_df, time_df, output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function for preprocessing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test(train_df, test_df, n_prev):\n",
    "    input_feats = []\n",
    "    \n",
    "    #Appending the training data with test data records with date of specified no of forecasting dates\n",
    "    append_df = pd.concat([train_df, test_df[test_df[\"Date\"] == train_df[\"Date\"].max() + timedelta(days=1)]])\n",
    "    \n",
    "    #Sorting the Dataframe in ascending order of Country region, province state & Date\n",
    "    append_df.sort_values([\"Country_Region\", \"Province_State\", \"Date\"], ascending=[True, True, True], inplace=True)\n",
    "    \n",
    "    # Performing Shifting of Previous cases in the positive direction (downwards) for New cases & New Fatalities\n",
    "    for i in range(1, n_prev+1):\n",
    "        for feat in [\"NewCases\", \"NewFatalities\"]:\n",
    "            append_df[\"{}_prev_{}\".format(feat, i)] = append_df.groupby([\"Country_Region\", \"Province_State\"])[feat].shift(i)\n",
    "            input_feats.append(\"{}_prev_{}\".format(feat, i))\n",
    "            \n",
    "    # Adding a column of ForecastId if records are not having null values        \n",
    "    append_df = append_df[append_df[\"ForecastId\"].notnull()]\n",
    "    \n",
    "    #Converting the Province state & Country Region to Dummy/Indicator Variables ( which is a constant)\n",
    "    const_df = pd.get_dummies(append_df[[\"Province_State\", \"Country_Region\"]], drop_first=True)\n",
    "    \n",
    "    # Assigning already available data for previous no of days counting back from starting date of forecasting dates\n",
    "    time_df = append_df[input_feats]\n",
    "    time_df = time_df.values.reshape((append_df.shape[0],-1,2))\n",
    "    \n",
    "    return const_df, time_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define main preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covid_preprocess_function():\n",
    "        \n",
    "        # Read train and test datasets\n",
    "        train_df = pd.read_csv(train_data_path)\n",
    "        print(\"train_df shape: {0}\" .format(train_df.shape))\n",
    "        test_df = pd.read_csv(test_data_path)\n",
    "        print(\"train_df shape: {0}\" .format(test_df.shape))\n",
    "\n",
    "        # Check the NaN value status in each column of the Train data\n",
    "        # Checking whether no column except Province_State are having NaN values\n",
    "        train_df.apply(lambda col: col.isnull().value_counts(), axis=0)\n",
    "        test_df.apply(lambda col: col.isna().value_counts(), axis=0)\n",
    "\n",
    "        # Replace the values of NaN with \"\"\n",
    "        train_df[\"Province_State\"] = train_df[\"Province_State\"].fillna(\"\")\n",
    "        test_df[\"Province_State\"] = test_df[\"Province_State\"].fillna(\"\")\n",
    "\n",
    "        # Convert the Date column values to Pandas Datetime format\n",
    "        train_df[\"Date\"] = pd.to_datetime(train_df[\"Date\"])\n",
    "        test_df[\"Date\"] = pd.to_datetime(test_df[\"Date\"])\n",
    "\n",
    "        # Add New Columns for \"NewCases\" and Fill the Column with difference values from the previous rows\n",
    "        train_df[\"NewCases\"] = train_df.groupby([\"Country_Region\", \"Province_State\"])[\"ConfirmedCases\"].diff(periods=1)\n",
    "\n",
    "        # Replace \"NewCases\" NaN values with 0\n",
    "        train_df[\"NewCases\"] = train_df[\"NewCases\"].fillna(0)\n",
    "\n",
    "        # Ensure that the NewCases are not negative. If NewCases are negative then they are replaced with zero else the actual value is provided\n",
    "        train_df[\"NewCases\"] = np.where(train_df[\"NewCases\"] < 0, 0, train_df[\"NewCases\"])\n",
    "\n",
    "        # Add a column for \"NewFatalities\" same as for \"NewCases\"\n",
    "        train_df[\"NewFatalities\"] = train_df.groupby([\"Country_Region\", \"Province_State\"])[\"Fatalities\"].diff(periods=1)\n",
    "        train_df[\"NewFatalities\"] = train_df[\"NewFatalities\"].fillna(0)\n",
    "        train_df[\"NewFatalities\"] = np.where(train_df[\"NewFatalities\"] < 0, 0, train_df[\"NewFatalities\"])\n",
    "\n",
    "        # Apply Natural Logarithmic Function to NewCases and NewFatalities Column\n",
    "        train_df[\"NewCases\"] = np.log(train_df[\"NewCases\"] + 1)\n",
    "        train_df[\"NewFatalities\"] = np.log(train_df[\"NewFatalities\"] + 1)\n",
    "        # print(\"train_df \\n\", train_df.head())\n",
    "        # print(\"test_df \\n\", test_df.head())\n",
    "        \n",
    "        #Calculate the number of days for which forecasting of New Cases & New Fatalities needs to be performed\n",
    "        n_next = (test_df[\"Date\"].max() - train_df[\"Date\"].max()).days\n",
    "        #print(\"No of Future Days requested to forecast COVID-19 New Cases & New Fatalities:\", n_next)\n",
    "        \n",
    "        const_df, time_df, output_df = preprocess_train(train_df, n_next, n_next)\n",
    "        \n",
    "        const_test_df, time_test_df = preprocess_test(train_df, test_df, n_next)\n",
    "        \n",
    "        return (train_df, test_df, const_df, time_df, output_df, const_test_df, time_test_df)\n",
    "        \n",
    "        \n",
    "train_df, test_df, const_df, time_df, output_df, const_test_df, time_test_df = covid_preprocess_function()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function for creating & training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covid_model_train_function(model_export_path, epochs, batch_size):\n",
    "    \n",
    "    time_input = Input(shape=(time_df.shape[1], time_df.shape[2]))\n",
    "    lstm = layers.LSTM(64)(time_input)\n",
    "\n",
    "    const_input = Input(shape=(const_df.shape[1],))\n",
    "\n",
    "    combine = layers.concatenate([lstm, const_input], axis=-1)\n",
    "    #lstm_out = layers.Dropout(0.1)(combine)\n",
    "    output = layers.Dense(output_df.shape[1], activation='softmax')(combine)\n",
    "\n",
    "    model = Model([time_input, const_input], output)\n",
    "    #optimizer=optimizers.SGD(lr=0.01, nesterov=True)\n",
    "    model.compile(optimizer='adam',loss='mean_squared_error',metrics=['acc'])\n",
    "    model.summary()\n",
    "    \n",
    "    model.fit([time_df, const_df], output_df, epochs=epochs, batch_size=batch_size)\n",
    "    \n",
    "    \n",
    "    input_names = ['input1','input2']\n",
    "    name_to_input = {name: t_input for name, t_input in zip(input_names, model.inputs)}\n",
    "    \n",
    "\n",
    "    tf.saved_model.simple_save(\n",
    "        keras.backend.get_session(),\n",
    "        os.path.join(model_export_path, \"001\"),\n",
    "        inputs=name_to_input,\n",
    "        outputs={t.name: t for t in model.outputs})    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define COVID class to be used for Kubeflow Fairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovidServe(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "            self.model = None\n",
    "            self.model_export_path = 'covid-model'\n",
    "            self.epochs = 5\n",
    "            self.batch_size = 64\n",
    "                 \n",
    "\n",
    "    def train(self):\n",
    "         covid_model_train_function(self.model_export_path, self.epochs, self.batch_size)\n",
    "         \n",
    "    def predict(self,X,feature_names=None):\n",
    "         \n",
    "        path=os.path.join(os.getcwd(), '/mnt/covid-model')\n",
    "        for dir in os.listdir(path):\n",
    "            if re.match('[0-9]',dir):\n",
    "                exported_path=os.path.join(path,dir)\n",
    "                break\n",
    "                \n",
    "        global output_dict\n",
    "        import logging\n",
    "        # Open a Session to predict\n",
    "        with tf.Session() as sess:\n",
    "            tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], exported_path)\n",
    "            predictor= tf.contrib.predictor.from_saved_model(exported_path,signature_def_key='serving_default')\n",
    "            input_data1=[]\n",
    "            input_data2=[]\n",
    "            for i in range(len(X[0])):\n",
    "                input_data1.append(X[0][i])\n",
    "                input_data2.append(X[1][i])\n",
    "            output_dict= predictor({\"input1\": input_data1, \"input2\": input_data2})\n",
    "            \n",
    "        sess.close()\n",
    "        return output_dict[\"dense_1/Softmax:0\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train COVID model on Kubeflow using Kubeflow Fairing\n",
    "Kubeflow Fairing packages the CovidServe class, the training data, and the training job's software prerequisites as a Docker image. Then Kubeflow Fairing deploys and runs the training job on kubeflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_job = TrainJob(CovidServe, input_files=[train_data_path, test_data_path,\"requirements.txt\"],\n",
    "                     pod_spec_mutators = [mounting_pvc(pvc_name=pvc, pvc_mount_path=\"/mnt/\")],\n",
    "                     docker_registry=DOCKER_REGISTRY, backend=BackendClass(build_context_source=minio_context_source))\n",
    "train_job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy trained model to Kubeflow for predictions using Kubeflow Fairing\n",
    "Kubeflow Fairing packages the CovidServe class, the trained model, and the prediction endpoint's software prerequisites as a Docker image. Then Kubeflow Fairing deploys and runs the prediction endpoint on Kubeflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = PredictionEndpoint(CovidServe, input_files=[train_data_path, test_data_path,\"requirements.txt\"],\n",
    "                              docker_registry=DOCKER_REGISTRY,\n",
    "                              pod_spec_mutators = [mounting_pvc(pvc_name=pvc, pvc_mount_path=\"/mnt/\")],\n",
    "                              backend=BackendClass(build_context_source=minio_context_source))\n",
    "endpoint.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain URL of prediction endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = endpoint.url\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design input data for prediction using prediction endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data={\"data\":{\"ndarray\":[time_test_df.tolist(), const_test_df.values.tolist()]}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using prediction endpoint\n",
    "Please wait for few mins to execute this step, as Prediction Endpoint Deployer Pod may take sometime to be up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "response=requests.post(url, data={'json':json.dumps(input_data)})\n",
    "predictions=response.json()[\"data\"][\"ndarray\"]\n",
    "print(len(predictions))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing prediction results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter the part of test data with specified future dates and convert the exponential format back to normal values\n",
    "\n",
    "Also concatenating the predicted new cases & new fatalities to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_test_df = test_df[test_df[\"Date\"] > train_df[\"Date\"].max()]\n",
    "sub_test_df = pd.concat([sub_test_df,\n",
    "                         pd.DataFrame(np.array(predictions).reshape((-1, 2)), columns=[\"NewCases\", \"NewFatalities\"], index=sub_test_df.index)],\n",
    "                         axis=1)\n",
    "sub_test_df[\"NewCases\"] = np.exp(sub_test_df[\"NewCases\"]) - 1\n",
    "sub_test_df[\"NewFatalities\"] = np.exp(sub_test_df[\"NewFatalities\"]) - 1\n",
    "# sub_test_df.head()\n",
    "\n",
    "# Filtering the part of test data with available dates and merging leftout dates if any\n",
    "fixed_test_df = test_df[test_df[\"Date\"] <= train_df[\"Date\"].max()].merge(train_df[train_df[\"Date\"] >= test_df[\"Date\"].min()][[\"Province_State\",\"Country_Region\", \"Date\", \"ConfirmedCases\", \"Fatalities\"]],\n",
    "                                                                         how=\"left\", on=[\"Province_State\",\"Country_Region\", \"Date\"])\n",
    "fixed_test_df\n",
    "\n",
    "# Concatenate the fixed_test data with the sub_test data ( which includes predicted New cases & New Fatalities)\n",
    "predict_df = pd.concat([sub_test_df, fixed_test_df]).sort_values([\"Country_Region\", \"Province_State\", \"Date\"],\n",
    "                                                                 ascending=[True, True, True])\n",
    "\n",
    "# Add a new Index column to Predict_df\n",
    "predict_df = predict_df.reset_index()\n",
    "\n",
    "# Replace null values of Confirmed Cases & Fatalities using New Cases & New Fatalities by accumulation\n",
    "for i in range(len(predict_df)):\n",
    "    if pd.isnull(predict_df.iloc[i][\"ConfirmedCases\"]):\n",
    "        predict_df.loc[i, \"ConfirmedCases\"] = predict_df.iloc[i - 1][\"ConfirmedCases\"] + predict_df.iloc[i][\"NewCases\"]\n",
    "    if pd.isnull(predict_df.iloc[i][\"Fatalities\"]):\n",
    "        predict_df.loc[i, \"Fatalities\"] = predict_df.iloc[i - 1][\"Fatalities\"] + predict_df.iloc[i][\"NewFatalities\"]\n",
    "\n",
    "# Ensure shape of Prediction result is same with test data\n",
    "assert predict_df.shape[0] == test_df.shape[0]\n",
    "\n",
    "# Get the Prediction result of Confirmed Cases & Fatalities for the specified future dates\n",
    "predict_df[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]].to_csv(\"submission.csv\", index=False)\n",
    "predict_df[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter India's data & visualise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise Plotly plot showing the overall trend of increasing confirmed cases right from the beginning of spread till the future date\n",
    "\n",
    "The intersection of the green and the red line shows the breaking point whether the cases starts to increase drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = \"India\"\n",
    "\n",
    "target = \"ConfirmedCases\"\n",
    "region_train_df = train_df[(train_df[\"Country_Region\"]==country)]\n",
    "region_predict_df = predict_df[(predict_df[\"Country_Region\"]==country)]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "ax1 = fig.add_axes([0, 0, 1, 1])\n",
    "ax1.plot(region_train_df[\"Date\"],\n",
    "         region_train_df[target],\n",
    "         color=\"green\")\n",
    "\n",
    "ax1.plot(region_predict_df[\"Date\"],\n",
    "         region_predict_df[target],\n",
    "         color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete prediction endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
